services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    deploy:
      resources:
        limits:
          memory: 4G    # Hard cap: Ollama cannot exceed 4GB
          cpus: '2.0'   # Limit to 2 CPU cores (out of 4) to keep SSH responsive
        reservations:
          memory: 2G    # Guaranteed minimum
    ports:
      - "11434:11434"
    env_file:
      - .env
      - ../../private.env
    volumes:
      - ${ROOT}${MEDIA}/ollama:/root/.ollama
      - ./Modelfile:/Modelfile
    environment:
      - OLLAMA_KEEP_ALIVE=5m  # Unload model after 5 mins of inactivity
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=1
    # Automated Pull and Build logic
    entrypoint: [ "/bin/sh", "-c", "ollama serve & sleep 15 && ollama pull llama3.2:3b-instruct-q4_K_M && ollama create budget-ai -f /Modelfile && wait" ]
    restart: unless-stopped

volumes: {}
